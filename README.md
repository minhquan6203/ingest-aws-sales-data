# ğŸš€ AWS Sales Data ETL Pipeline

A comprehensive **Extract, Load, Transform (ELT)** system built with modern 3-layer architecture for processing AWS sales data efficiently using Apache Spark and PostgreSQL.

## ğŸ“‹ Table of Contents

- [ğŸ—ï¸ Architecture Overview](#ï¸-architecture-overview)
- [âœ¨ Key Features](#-key-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ’» Usage Examples](#-usage-examples)
- [ğŸ“Š Data Assets](#-data-assets)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ” Monitoring & Analytics](#-monitoring--analytics)
- [ğŸ³ Docker Deployment](#-docker-deployment)
- [ğŸ¯ Business Intelligence](#-business-intelligence)

## ğŸ—ï¸ Architecture Overview

```
Data Sources â†’ Bronze Layer â†’ Silver Layer â†’ Gold Layer â†’ Analytics
   (Raw)      (Validated)    (Cleaned)     (Business)    (Reports)
```

### Layer Breakdown

- **ğŸ¥‰ Bronze Layer**: Raw data ingestion with validation and quality checks
- **ğŸ¥ˆ Silver Layer**: Cleaned, standardized, and validated data
- **ğŸ¥‡ Gold Layer**: Business-ready dimensional model with fact/dimension tables
- **ğŸ“Š Analytics Layer**: Advanced analytics, dashboards, and business intelligence

## âœ¨ Key Features

### ğŸ¯ Advanced Data Processing
- **Incremental & Full Load**: Support for both incremental and full data loads
- **Distributed Processing**: Apache Spark-based for handling large datasets
- **Data Quality**: Comprehensive validation rules and quality checks
- **Error Handling**: Robust error handling with detailed logging

### ğŸ“ˆ Business Intelligence
- **Dimensional Modeling**: Star schema with fact and dimension tables
- **Advanced Analytics**: Sales performance metrics, funnel analysis, and trends
- **Executive Dashboard**: Real-time KPIs and business insights
- **Lead Scoring**: Intelligent lead qualification and scoring

### ğŸ”§ Production Ready
- **Audit Trail**: Complete tracking of all pipeline executions
- **Monitoring**: Real-time monitoring and alerting capabilities
- **Containerization**: Docker and Docker Compose support
- **Scheduling**: Built-in pipeline scheduling capabilities

## ğŸš€ Quick Start with Docker

### CÃ¡ch 1: Sá»­ dá»¥ng Docker Compose (KhuyÃªn dÃ¹ng)
```bash
# Clone repository
git clone <repository-url>
cd ingest-aws-sales-data

# Khá»Ÿi Ä‘á»™ng táº¥t vá»›i Docker Compose
docker-compose up --build
```

### CÃ¡ch 2: Sá»­ dá»¥ng PowerShell Script (Windows)
```bash
# Cháº¡y script PowerShell (tá»± Ä‘á»™ng build vÃ  start)
.\run_docker.ps1
```

### CÃ¡ch 3: Sá»­ dá»¥ng Batch Script (Windows)
```bash
# Cháº¡y script batch
.\run_docker.bat
```

### Cháº¡y trong Background
```bash
# Cháº¡y dá»‹ch vá»¥ trong background
docker-compose up -d --build

# Xem tráº¡ng thÃ¡i dá»‹ch vá»¥
docker-compose ps

# Xem logs
docker-compose logs -f aws-sales-etl

# Dá»«ng dá»‹ch vá»¥
docker-compose down
```

## ğŸ’» Sá»­ dá»¥ng Docker Container

### Cháº¡y Pipeline trong Docker
```bash
# Cháº¡y pipeline AWS sales chuáº©n (8 bÆ°á»›c)
docker-compose exec aws-sales-etl python src/main.py --run-aws-sales

# Cháº¡y pipeline nÃ¢ng cao vá»›i analytics (11 bÆ°á»›c)
docker-compose exec aws-sales-etl python src/main.py --run-enhanced-aws-sales

# Cháº¡y vá»›i full load thay vÃ¬ incremental
docker-compose exec aws-sales-etl python src/main.py --run-enhanced-aws-sales --full-load

# Cháº¡y pipeline cá»¥ thá»ƒ
docker-compose exec aws-sales-etl python src/main.py --run-pipeline aws_sales_pipeline

# Liá»‡t kÃª táº¥t cáº£ pipeline cÃ³ sáºµn
docker-compose exec aws-sales-etl python src/main.py --list-pipelines
```

### Analytics Operations trong Docker
```bash
# Cháº¡y analytics pipeline cá»¥ thá»ƒ
docker-compose exec aws-sales-etl python src/main.py --run-analytics-pipeline performance
docker-compose exec aws-sales-etl python src/main.py --run-analytics-pipeline funnel
docker-compose exec aws-sales-etl python src/main.py --run-analytics-pipeline trends
docker-compose exec aws-sales-etl python src/main.py --run-analytics-pipeline scoring

# Táº¡o analytics dashboard
docker-compose exec aws-sales-etl python src/main.py --run-analytics

# Hiá»ƒn thá»‹ schema dimensional
docker-compose exec aws-sales-etl python src/main.py --show-schema
```

### Data Exploration trong Docker
```bash
# Xem tá»•ng quan táº¥t cáº£ tables trong warehouse
docker-compose exec aws-sales-etl python src/main.py --show-all-tables

# Xem dá»¯ liá»‡u Bronze layer
docker-compose exec aws-sales-etl python src/main.py --show-bronze-data

# Xem dá»¯ liá»‡u Silver layer
docker-compose exec aws-sales-etl python src/main.py --show-silver-data

# Xem dá»¯ liá»‡u Gold layer  
docker-compose exec aws-sales-etl python src/main.py --show-gold-data

# Xem dá»¯ liá»‡u table cá»¥ thá»ƒ
docker-compose exec aws-sales-etl python src/main.py --show-table gold_fact_sales

# Xem nhiá»u hÆ¡n 10 rows (default)
docker-compose exec aws-sales-etl python src/main.py --show-table gold_fact_sales --limit 20
```

### Monitoring & Debugging trong Docker
```bash
# Xem lá»‹ch sá»­ thá»±c thi pipeline
docker-compose exec aws-sales-etl python src/main.py --show-history

# Test data ingestion
docker-compose exec aws-sales-etl python src/main.py --test-ingestion

# Test audit functionality
docker-compose exec aws-sales-etl python src/main.py --test-audit

# Xem logs cá»§a container
docker-compose logs -f aws-sales-etl

# VÃ o shell cá»§a container
docker-compose exec aws-sales-etl bash
```

### Quáº£n lÃ½ Docker Services
```bash
# Kiá»ƒm tra tráº¡ng thÃ¡i cÃ¡c dá»‹ch vá»¥
docker-compose ps

# Restart má»™t dá»‹ch vá»¥ cá»¥ thá»ƒ
docker-compose restart aws-sales-etl

# Xem resource usage
docker stats

# Dá»n dáº¹p
docker-compose down --volumes --rmi all
```

## ğŸ“Š Data Assets

### Bronze Layer
- `bronze_aws_sales` - Raw sales data with validation and data quality checks

### Silver Layer  
- `silver_aws_sales` - Cleaned, standardized, and validated sales data

### Gold Layer - Dimensional Model

#### Dimension Tables
- `gold_dim_salesperson` - Salesperson master data
- `gold_dim_lead` - Lead/Customer dimension with segmentation
- `gold_dim_date` - Date dimension with calendar hierarchies
- `gold_dim_opportunity_stage` - Sales stage definitions

#### Fact Tables
- `gold_fact_sales` - Central sales fact table with all metrics

#### Business Views
- `gold_aws_sales_summary` - Executive sales summaries by salesperson
- `gold_aws_sales_by_region` - Regional performance analysis

### Gold Layer - Advanced Analytics
- `gold_sales_performance_metrics` - Sales performance KPIs and metrics
- `gold_opportunity_funnel` - Sales funnel analysis and conversion rates
- `gold_monthly_sales_trends` - Time-series analysis and forecasting
- `gold_lead_scoring` - Lead qualification and scoring insights

## ğŸ› ï¸ YÃªu cáº§u há»‡ thá»‘ng

### Prerequisites
- **Docker** (phiÃªn báº£n 20.10+)
- **Docker Compose** (phiÃªn báº£n 2.0+)
- **Git** Ä‘á»ƒ clone repository
- **8GB RAM** tá»‘i thiá»ƒu (16GB khuyáº¿n nghá»‹)
- **10GB dung lÆ°á»£ng á»• cá»©ng** trá»‘ng

### Kiá»ƒm tra Docker
```bash
# Kiá»ƒm tra Docker version
docker --version

# Kiá»ƒm tra Docker Compose version
docker-compose --version

# Test Docker hoáº¡t Ä‘á»™ng
docker run hello-world
```

### Setup ban Ä‘áº§u
```bash
# Clone repository
git clone <repository-url>
cd ingest-aws-sales-data

# Táº¡o thÆ° má»¥c cáº§n thiáº¿t (tá»± Ä‘á»™ng táº¡o khi cháº¡y Docker)
mkdir -p data logs

# File SalesData.csv sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº£i vá» khi cháº¡y pipeline
```

## ğŸ”§ Cáº¥u hÃ¬nh Docker

### Cáº¥u hÃ¬nh trong docker-compose.yml
Táº¥t cáº£ cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p sáºµn trong file `docker-compose.yml`:

```yaml
# CÃ¡c dá»‹ch vá»¥ Ä‘Æ°á»£c cáº¥u hÃ¬nh:
services:
  postgres:         # PostgreSQL Database
    ports: "5432:5432"
    database: etl_demo
    
  minio:           # Object Storage (tÆ°Æ¡ng thÃ­ch S3)
    ports: "9000:9000, 9001:9001"
    
  aws-sales-etl:   # Main ETL Application
    depends_on: postgres, minio
```

### Thay Ä‘á»•i cáº¥u hÃ¬nh (náº¿u cáº§n)
```bash
# Chá»‰nh sá»­a file docker-compose.yml
# Thay Ä‘á»•i ports, passwords, hoáº·c volumes theo nhu cáº§u

# Restart sau khi thay Ä‘á»•i
docker-compose down
docker-compose up --build
```

### Truy cáº­p cÃ¡c dá»‹ch vá»¥
- **PostgreSQL**: `localhost:5432` (user: postgres, pass: postgres)
- **MinIO Console**: `http://localhost:9001` (admin/password123)
- **MinIO API**: `http://localhost:9000`

## ğŸ“ Project Structure

```
ingest-aws-sales-data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Main entry point
â”‚   â”œâ”€â”€ analytics/              # Analytics dashboard and reporting
â”‚   â”‚   â””â”€â”€ dashboard.py        # Executive dashboard generator
â”‚   â”œâ”€â”€ audit/                  # Audit trail system
â”‚   â”‚   â””â”€â”€ audit.py           # Pipeline execution tracking
â”‚   â”œâ”€â”€ config/                 # Configuration and metadata
â”‚   â”‚   â”œâ”€â”€ metadata.py        # Pipeline and data source definitions
â”‚   â”‚   â””â”€â”€ config.py          # System configuration
â”‚   â”œâ”€â”€ controller/             # ETL orchestration
â”‚   â”‚   â””â”€â”€ etl_controller.py  # Main controller for pipeline execution
â”‚   â”œâ”€â”€ ingestion/              # Bronze layer data ingestion
â”‚   â”‚   â””â”€â”€ csv_ingestion.py   # CSV file ingestion logic
â”‚   â”œâ”€â”€ transformation/         # Silver & Gold layer transformations
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py # Data cleaning and standardization
â”‚   â”‚   â””â”€â”€ silver_to_gold.py   # Dimensional modeling
â”‚   â”œâ”€â”€ notification/           # Alerting and notifications
â”‚   â””â”€â”€ utils/                  # Utilities and helpers
â”‚       â”œâ”€â”€ spark_utils.py     # Spark session management
â”‚       â””â”€â”€ db_utils.py        # Database utilities
â”œâ”€â”€ data/                       # Data files (SalesData.csv)
â”œâ”€â”€ logs/                       # Execution logs
â”œâ”€â”€ docker-compose.yml          # Container orchestration
â”œâ”€â”€ Dockerfile                  # Application container
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ run_docker.ps1             # PowerShell deployment script
â”œâ”€â”€ run_docker.bat             # Batch deployment script
â””â”€â”€ README.md                  # This file
```

## ğŸ” Monitoring & Analytics

### Execution Monitoring
- **Audit Trail**: Every pipeline execution is tracked with timestamps, record counts, and status
- **Error Logging**: Detailed error logging with stack traces
- **Performance Metrics**: Execution duration and resource usage tracking

### Analytics Dashboard Features
- **Executive Summary**: High-level KPIs and performance metrics
- **Sales Performance**: Individual and team performance analysis
- **Opportunity Funnel**: Conversion rates and pipeline health
- **Regional Analysis**: Geographic performance breakdown
- **Trend Analysis**: Time-series analysis and forecasting
- **Lead Scoring**: Lead qualification and prioritization

### Sample Analytics Queries
```sql
-- Top performing salespeople
SELECT salesperson, region, actual_revenue, win_rate_percent
FROM gold_sales_performance_metrics
ORDER BY actual_revenue DESC;

-- Sales funnel analysis
SELECT stage, opportunity_count, stage_pipeline_value, conversion_rate
FROM gold_opportunity_funnel
ORDER BY stage;

-- Monthly revenue trends
SELECT year, month, region, revenue_won, revenue_pipeline
FROM gold_monthly_sales_trends
ORDER BY year DESC, month DESC;

-- Lead scoring insights
SELECT segment, avg_score, high_value_leads, conversion_probability
FROM gold_lead_scoring
ORDER BY avg_score DESC;
```

## ğŸ³ Docker Deployment

### Services Architecture
```yaml
services:
  postgres:      # Data warehouse
  minio:         # Object storage (S3-compatible)
  aws-sales-etl: # Main ETL application
```

### Quick Deployment
```bash
# Start all services
docker-compose up --build

# Run in background
docker-compose up -d --build

# Check service status
docker-compose ps

# View logs
docker-compose logs aws-sales-etl

# Stop services
docker-compose down
```

### Service Access
- **PostgreSQL**: `localhost:5432`
- **MinIO Console**: `http://localhost:9001`
- **Application Logs**: Available in `logs/` directory

## ğŸ¯ Business Intelligence

### Key Performance Indicators (KPIs)
- **Revenue Metrics**: Actual vs forecasted revenue, win rates
- **Sales Performance**: Individual and team performance rankings
- **Pipeline Health**: Opportunity counts, stage distribution
- **Conversion Rates**: Stage-to-stage conversion analysis
- **Lead Quality**: Lead scoring and qualification metrics

### Executive Dashboard Export
The system generates comprehensive business reports in CSV format:
- Executive summary with key metrics
- Sales performance by salesperson
- Regional performance analysis
- Opportunity funnel metrics
- Monthly trend analysis

### Advanced Analytics Features
- **Predictive Analytics**: Lead scoring and revenue forecasting
- **Trend Analysis**: Historical trend analysis and seasonality detection
- **Performance Benchmarking**: Comparative analysis across regions and teams
- **Pipeline Forecasting**: Future revenue and opportunity projections

## ğŸ”§ Troubleshooting

### Common Issues

1. **SalesData.csv not found**
   - The pipeline automatically downloads the file from AWS samples
   - Ensure internet connectivity for automatic download

2. **Spark connection issues**
   - Check if Spark is properly installed
   - Verify JAVA_HOME environment variable

3. **Database connection errors**
   - Ensure PostgreSQL is running
   - Check connection parameters in docker-compose.yml

4. **Memory issues with large datasets**
   - Adjust Spark configuration in `src/utils/spark_utils.py`
   - Increase Docker memory allocation

### Logging
- All execution logs are stored in `logs/` directory
- Use `--show-history` to view pipeline execution history
- Check Docker logs with `docker-compose logs`

## ğŸ“§ Support

For issues, questions, or contributions:
1. Check the troubleshooting section above
2. Review execution logs in the `logs/` directory
3. Use `python src/main.py --help` for usage information
4. Run `python src/main.py --test-ingestion` to verify data access

## ğŸ“„ License

This project is provided as-is for educational and demonstration purposes.

---

**Built with â¤ï¸ using Apache Spark, PostgreSQL, and modern data engineering practices.** 